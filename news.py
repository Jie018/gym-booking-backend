import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import hashlib
import time
from datetime import datetime
from pathlib import Path
import re

class PUSportsMonitor:
    """éœå®œå¤§å­¸é«”è‚²å®¤æ–°èç›£æ§ç³»çµ±ï¼ˆHTML è§£æç‰ˆï¼‰"""
    def __init__(self):
        self.base_url = "https://b023.pu.edu.tw"
        self.main_url = "https://b023.pu.edu.tw/p/403-1049-882.php?Lang=zh-tw"
        self.headers = {"User-Agent": "Mozilla/5.0"}
        self.data_dir = Path("pu_sports_data")
        self.data_dir.mkdir(exist_ok=True)
        self.history_file = self.data_dir / "news_history.json"
        self.last_hash = None
        self.news_cache = []

    def fetch_html(self, url):
        try:
            resp = requests.get(url, headers=self.headers, timeout=15)
            resp.raise_for_status()
            resp.encoding = 'utf-8'
            return resp.text
        except Exception as e:
            print(f"âŒ ç¶²è·¯éŒ¯èª¤: {e}")
            return None

    def parse_news(self, html):
        soup = BeautifulSoup(html, 'html.parser')
        news_list = []

        # åµæ¸¬æ–°èé …ç›®
        items = soup.find_all('div', class_=re.compile(r'(d-item|item|news)', re.I))
        if not items:
            items = soup.find_all('li')  # fallback
        
        for item in items:
            try:
                a_tag = item.find('a', href=True)
                if not a_tag:
                    continue
                title = a_tag.get_text(strip=True)
                href = a_tag['href']
                if href.startswith('/'):
                    url = self.base_url + href
                elif not href.startswith('http'):
                    url = self.base_url + '/' + href
                else:
                    url = href

                # æ‰¾æ—¥æœŸ
                date_match = re.search(r'(\d{4})[-/](\d{1,2})[-/](\d{1,2})', item.get_text())
                date = "æœªçŸ¥"
                if date_match:
                    date = f"{date_match.group(1)}-{date_match.group(2).zfill(2)}-{date_match.group(3).zfill(2)}"

                # åˆ¤æ–·é¡åˆ¥
                category = "å…¬å‘Š"
                if any(k in title for k in ["é«”è‚²é¤¨", "å ´é¤¨"]):
                    category = "é«”è‚²å ´é¤¨"
                elif any(k in title for k in ["æ•™å­¸", "ç ”ç¿’"]):
                    category = "æ•™å­¸ç ”ç¿’"
                elif any(k in title for k in ["æ´»å‹•", "è¨“ç·´", "ç«¶è³½"]):
                    category = "æ´»å‹•è¨“ç·´"

                if title and url:
                    news_list.append({
                        "æ—¥æœŸ": date,
                        "æ¨™é¡Œ": title,
                        "é€£çµ": url,
                        "é¡åˆ¥": category,
                        "æŠ“å–æ™‚é–“": datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
            except Exception:
                continue

        # å»é‡
        seen = set()
        unique_news = []
        for n in news_list:
            if n["é€£çµ"] not in seen:
                seen.add(n["é€£çµ"])
                unique_news.append(n)

        return unique_news

    def get_hash(self, obj):
        j = json.dumps(obj, sort_keys=True, ensure_ascii=False)
        return hashlib.md5(j.encode("utf-8")).hexdigest()

    def load_history(self):
        if self.history_file.exists():
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    obj = json.load(f)
                    self.last_hash = obj.get("last_hash")
                    self.news_cache = obj.get("news", [])
                    return True
            except Exception as e:
                print(f"âš ï¸ è¼‰å…¥æ­·å²è¨˜éŒ„éŒ¯èª¤: {e}")
        return False

    def save_history(self, news_list, page_hash):
        obj = {
            "last_hash": page_hash,
            "last_update": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "news": news_list
        }
        with open(self.history_file, 'w', encoding='utf-8') as f:
            json.dump(obj, f, ensure_ascii=False, indent=2)

    def detect_new(self, current_news):
        if not self.news_cache:
            return current_news
        old_urls = {n["é€£çµ"] for n in self.news_cache}
        return [n for n in current_news if n["é€£çµ"] not in old_urls]

    def save_to_files(self, news_list):
        if not news_list:
            return
        ts = datetime.now().strftime('%Y%m%d_%H%M%S')
        df = pd.DataFrame(news_list)
        csv_file = self.data_dir / f"pu_sports_news_{ts}.csv"
        df.to_csv(csv_file, index=False, encoding='utf-8-sig')
        json_file = self.data_dir / f"pu_sports_news_{ts}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(news_list, f, ensure_ascii=False, indent=2)
        print(f"âœ“ å·²å„²å­˜: {csv_file.name} / {json_file.name}")

    def display(self, news_list, title="æ–°å¢æ¶ˆæ¯"):
        if not news_list:
            return
        print(f"\n{'='*70}")
        print(f"ğŸ“° {title} (å…± {len(news_list)} å‰‡)")
        print(f"{'='*70}")
        for i, n in enumerate(news_list, 1):
            print(f"{i:2d}. [{n['é¡åˆ¥']}] {n['æ—¥æœŸ']} - {n['æ¨™é¡Œ']}")
            print(f"    é€£çµ: {n['é€£çµ']}")
            print()

    def run_once(self):
        print(f"\n{'='*70}")
        print(f"ğŸ” é–‹å§‹æª¢æŸ¥: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}")

        self.load_history()
        html = self.fetch_html(self.main_url)
        if html is None:
            print("âŒ ç„¡æ³•å–å¾—ç¶²é è³‡æ–™")
            return

        news_list = self.parse_news(html)
        if not news_list:
            print("âš ï¸ æ²’æŠ“åˆ°ä»»ä½•æ–°èï¼Œè«‹æª¢æŸ¥ç¶²ç«™çµæ§‹")
            debug_file = self.data_dir / f"debug_html_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(html)
            print(f"ğŸ“„ HTMLå·²å„²å­˜è‡³ {debug_file} ä¾›æª¢æŸ¥")
            return

        print(f"âœ“ æˆåŠŸè§£æ {len(news_list)} å‰‡æ¶ˆæ¯")
        page_hash = self.get_hash(news_list)

        if page_hash == self.last_hash:
            print("âœ“ è³‡æ–™ç„¡è®ŠåŒ–")
        else:
            new_items = self.detect_new(news_list)
            if new_items:
                self.display(new_items, "æ–°å¢æ¶ˆæ¯")
            else:
                print("âœ“ æœ‰æ›´æ–°ä½†ç„¡æ–°å¢é …ç›®")

            # å„²å­˜çˆ¬èŸ²åŸå§‹è³‡æ–™
            self.save_to_files(news_list)
            self.save_history(news_list, page_hash)

            # --- ç”Ÿæˆå‰ç«¯å¯è®€ JSON ---
            frontend_news = [
                {
                    "date": n["æ—¥æœŸ"],
                    "title": n["æ¨™é¡Œ"],
                    "url": n["é€£çµ"],
                    "category": n["é¡åˆ¥"]
                } for n in news_list
            ]
            frontend_file = Path("data/latest_news.json")  # å‰ç«¯å¯è®€è³‡æ–™è·¯å¾‘
            frontend_file.parent.mkdir(exist_ok=True)
            with open(frontend_file, "w", encoding="utf-8") as f:
                json.dump(frontend_news, f, ensure_ascii=False, indent=2)
            print(f"âœ“ å‰ç«¯ JSON å·²æ›´æ–°: {frontend_file}")

        print(f"{'='*70}\nâœ… æª¢æŸ¥å®Œæˆ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")

    def run_monitor(self, interval_days=7):
        print(f"{'='*70}")
        print("ğŸ¤– éœå®œå¤§å­¸é«”è‚²å®¤æ–°èç›£æ§ç³»çµ±å•Ÿå‹•")
        print(f"{'='*70}")
        print(f"â° ç›£æ§é–“éš”: {interval_days} å¤©")
        print(f"ğŸ”— ç¶²å€: {self.main_url}")
        print(f"{'='*70}")
        self.run_once()
        try:
            while True:
                time.sleep(interval_days * 24 * 3600)
                self.run_once()
        except KeyboardInterrupt:
            print("\nâœ‹ åœæ­¢ç›£æ§")

# ä¸»ç¨‹å¼
if __name__ == "__main__":
    monitor = PUSportsMonitor()
    print("="*70)
    print("éœå®œå¤§å­¸é«”è‚²å®¤æ–°èç›£æ§ç³»çµ±")
    print("="*70)
    print("1. åŸ·è¡Œä¸€æ¬¡æª¢æŸ¥")
    print("2. æ¯é€±ç›£æ§ (é è¨­)")
    print("3. è‡ªè¨‚é–“éš”å¤©æ•¸")
    print("="*70)
    choice = input("è«‹é¸æ“‡ (1-3ï¼ŒEnter=é è¨­2): ").strip()
    if choice == "1":
        monitor.run_once()
    elif choice == "3":
        try:
            days = int(input("è«‹è¼¸å…¥å¤©æ•¸: ").strip())
            if days < 1:
                print("âŒ é–“éš”å¤©æ•¸å¿…é ˆ > 0")
            else:
                monitor.run_monitor(interval_days=days)
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
    else:
        monitor.run_monitor(interval_days=7)
